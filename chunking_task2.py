# -*- coding: utf-8 -*-
"""Chunking_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jAVzGdzVSlx0OOJh-lmsbgFXneK_qoFM
"""

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

def semantic_chunking(text):
    doc = nlp(text)

    # Split text into sentences
    sentences = [sent.text for sent in doc.sents]

    # Optional: Further chunk by paragraphs (if available in the text)
    # paragraphs = text.split("\n\n")

    return sentences  # You can also return paragraphs if working with paragraph structure

# Example usage
text = "You can enjoy. When you play this."
chunks = semantic_chunking(text)

# Print the sentences
for chunk in chunks:
    print(chunk)

!pip install scikit-learn sentence-transformers

from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer

def kmeans_chunking(text, num_clusters=5):
    # Convert text into sentences using a more robust method
    # such as spaCy (from previous examples)
    import spacy
    nlp = spacy.load("en_core_web_sm")  # Assuming you have spaCy installed
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]


    # If there's only one sentence, avoid clustering
    if len(sentences) < num_clusters:
        return sentences

    # Load a pre-trained sentence embedding model
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # Get sentence embeddings
    sentence_embeddings = model.encode(sentences)

    # Apply K-Means clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans.fit(sentence_embeddings)

    # Group sentences into clusters
    clustered_sentences = [[] for _ in range(num_clusters)]
    for i, label in enumerate(kmeans.labels_):
        clustered_sentences[label].append(sentences[i])

    # Create chunks by joining clustered sentences
    chunks = [" ".join(cluster) for cluster in clustered_sentences]

    return chunks

# Example usage
text = "You can enjoy. When you play this."
chunks = kmeans_chunking(text, num_clusters=5)

for idx, chunk in enumerate(chunks):
    print(f"Chunk {idx+1}:")
    print(chunk)
    print("\n")

!pip install langchain

from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter

def langchain_text_splitter(text, chunk_size=1000, chunk_overlap=200):
    # You can use either RecursiveCharacterTextSplitter or CharacterTextSplitter
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    # Split the text into chunks
    chunks = splitter.split_text(text)

    return chunks

# Example usage
text = "You can enjoy. When you play this."
chunks = langchain_text_splitter(text, chunk_size=1000, chunk_overlap=200)

for idx, chunk in enumerate(chunks):
    print(f"Chunk {idx+1}:")
    print(chunk)
    print("\n")

"""Task 3: Vectorization

Step 1: Choosing an Embedding Model
"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer

# Load a pre-trained SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model, fast for large datasets

def create_embeddings(text_chunks):
    # Convert each chunk into an embedding (vector)
    embeddings = model.encode(text_chunks)
    return embeddings

# Example usage
text_chunks = [
    "First chunk of legal text.",
    "Second chunk of legal text.",
    # Add more chunks as needed
]
embeddings = create_embeddings(text_chunks)

# Print the first embedding (vector representation)
print(embeddings[0])

"""Step 2: Storing Embeddings in a Vector Database

Using ChromaDB
"""

!pip install chromadb

import chromadb

# Create a ChromaDB client
client = chromadb.Client()

# Create a new collection (this is like a table in a traditional database)
collection = client.create_collection(name="legal_text_embeddings")

def store_embeddings_chromadb(text_chunks, embeddings):
    for i, embedding in enumerate(embeddings):
        collection.add(
            documents=[text_chunks[i]],  # Original text chunk
            embeddings=[embedding],      # Corresponding embedding
            ids=[str(i)]                 # Unique ID for each chunk
        )

# Store embeddings in the collection
store_embeddings_chromadb(text_chunks, embeddings)

# Example query: find similar chunks to a new input
query = "Find chunks related to constitutional law."
query_embedding = model.encode([query])
results = collection.query(query_embeddings=query_embedding, n_results=5)

print("Top 5 similar chunks:")
for result in results["documents"]:
    print(result)

